{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install -q transformers[torch] datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install -q bitsandbytes trl peft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(\"HuggingFaceH4/ultrachat_200k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to C:\\Users\\lucam\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['prompt', 'prompt_id', 'messages'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['prompt', 'prompt_id', 'messages'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "# remove this when done debugging\n",
    "indices = range(0,100)\n",
    "\n",
    "dataset_dict = {\"train\": raw_datasets[\"train_sft\"].select(indices),\n",
    "                \"test\": raw_datasets[\"test_sft\"].select(indices)}\n",
    "\n",
    "raw_datasets = DatasetDict(dataset_dict)\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['prompt', 'prompt_id', 'messages'])\n"
     ]
    }
   ],
   "source": [
    "example = raw_datasets[\"train\"][0]\n",
    "print(example.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user                :  These instructions apply to section-based themes (Responsive 6.0+, Retina 4.0+, Parallax 3.0+ Turbo 2.0+, Mobilia 5.0+). What theme version am I using?\n",
      "On your Collections pages & Featured Collections sections, you can easily show the secondary image of a product on hover by enabling one of the theme's built-in settings!\n",
      "Your Collection pages & Featured Collections sections will now display the secondary product image just by hovering over that product image thumbnail.\n",
      "Does this feature apply to all sections of the theme or just specific ones as listed in the text material?\n",
      "assistant           :  This feature only applies to Collection pages and Featured Collections sections of the section-based themes listed in the text material.\n",
      "user                :  Can you guide me through the process of enabling the secondary image hover feature on my Collection pages and Featured Collections sections?\n",
      "assistant           :  Sure, here are the steps to enable the secondary image hover feature on your Collection pages and Featured Collections sections:\n",
      "\n",
      "1. Log in to your Shopify account and go to your Online Store.\n",
      "2. Click on Customize theme for the section-based theme you are using.\n",
      "3. Navigate to the Collection pages or Featured Collections section where you want to enable the secondary image hover feature.\n",
      "4. With the section open, click on the section's setting (gear) icon in the top-left corner.\n",
      "5. In the settings panel that appears, look for an option labeled 'Image display' or 'Image hover'.\n",
      "6. If available, select 'Show secondary image on hover'.\n",
      "7. Save the changes and preview the Collection/Featured Collection page to see the effect.\n",
      "\n",
      "If you're having trouble finding the setting, the best thing to do is refer to your theme's documentation, since the location and labeling of settings can vary between themes.\n",
      "user                :  Can you provide me with a link to the documentation for my theme?\n",
      "assistant           :  I don't have access to your store's theme information. However, you can usually find the documentation for your theme by going to the shopify theme store, finding your theme and clicking on the 'support' link located in the bottom right corner of the page. Alternatively, you can do a google search for the name of your theme followed by 'documentation' or 'user guide'.\n",
      "user                :  Can you confirm if this feature also works for the Quick Shop section of my theme?\n",
      "assistant           :  The secondary image hover feature may or may not work for your Quick Shop section, depending on the configuration of your theme. Some themes include this feature in the Quick Shop section by default, while others may require additional customization. To check if this feature is available for the Quick Shop section of your theme, follow these steps:\n",
      "\n",
      "1. Go to the Quick Shop section where you would like to enable the feature. 2. Click on the Quick Shop settings icon (gear icon) and look for 'Image display' or 'Image hover'. 3. If available, select 'Show secondary image on hover'. 4. Save the changes. If this option is not available in your Quick Shop section settings, you may need to reach out to your theme developer for assistance with customizing your Quick Shop section to include this feature.\n"
     ]
    }
   ],
   "source": [
    "messages = example[\"messages\"]\n",
    "for message in messages:\n",
    "  role = message[\"role\"]\n",
    "  content = message[\"content\"]\n",
    "  print('{0:20}:  {1}'.format(role, content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# set pad_token_id equal to the eos_token_id if not set\n",
    "if tokenizer.pad_token_id is None:\n",
    "  tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Set reasonable default for models without max length\n",
    "if tokenizer.model_max_length > 100_000:\n",
    "  tokenizer.model_max_length = 2048\n",
    "\n",
    "# Set chat template\n",
    "DEFAULT_CHAT_TEMPLATE = \"{% for message in messages %}\\n{% if message['role'] == 'user' %}\\n{{ '<|user|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'system' %}\\n{{ '<|system|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'assistant' %}\\n{{ '<|assistant|>\\n'  + message['content'] + eos_token }}\\n{% endif %}\\n{% if loop.last and add_generation_prompt %}\\n{{ '<|assistant|>' }}\\n{% endif %}\\n{% endfor %}\"\n",
    "tokenizer.chat_template = DEFAULT_CHAT_TEMPLATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 60 of the processed training set:\n",
      "\n",
      "<|system|>\n",
      "</s>\n",
      "<|user|>\n",
      "What is the traditional dress worn for a wedding in Greece?</s>\n",
      "<|assistant|>\n",
      "The traditional dress worn for a wedding in Greece is called a \"Fustanella,\" which is a traditional Greek skirt and a white shirt, usually with a vest or jacket. This is worn by the groom and his male family members, while the bride and female family members wear a white dress or gown. The traditional dress is often complemented with accessories such as a headpiece, jewelry, and shoes.</s>\n",
      "<|user|>\n",
      "Can you tell me more about the traditional Greek wedding customs besides the dress?</s>\n",
      "<|assistant|>\n",
      "Sure, here are some traditional Greek wedding customs:\n",
      "\n",
      "1. Engagement: Before a Greek wedding, the couple typically gets engaged, also known as \"kroiazi.\" It is a formal announcement of their intention to get married in front of their family and friends.\n",
      "\n",
      "2. Pre-wedding customs: A day or two before the wedding, the couple often hosts a pre-wedding celebration called \"Krevati.\" The bride's bed is decorated with flowers, and her friends and relatives gather around to sing and dance while the groom places money and gifts on the bed.\n",
      "\n",
      "3. Wedding ceremony: The Greek Orthodox wedding ceremony is rich in tradition and symbolism. The couple exchange rings and crowns, signifying their union as king and queen of their home. The ceremony is performed in a church where the couple receives blessings from the priest, and prayers are offered for their future happiness.\n",
      "\n",
      "4. Reception: After the ceremony, the couple and their guests enjoy a reception with food, drinks, and dancing. Dancing is an important part of Greek culture, and traditional dances like the kalamatiano and tsamiko are popular at weddings.\n",
      "\n",
      "5. Money dance: Another tradition is the \"kalamatiano,\" the money dance. Guests pin money on the couple's clothes while dancing together. The money symbolizes wealth and prosperity for the newlyweds in their future life together.\n",
      "\n",
      "6. Throwing rice: It is traditional for guests to throw rice or koufeta (sugar-coated almonds) at the couple as they exit the church. It is believed to bring good luck, fertility and prosperity to the couple.\n",
      "\n",
      "These are just a few of the many Greek wedding customs that make the celebration unique and special.</s>\n",
      "<|user|>\n",
      "Wow, the Greek wedding customs are so fascinating! What other traditional customs do they have for other celebrations or events?</s>\n",
      "<|assistant|>\n",
      "Yes, Greek customs and traditions are very interesting and unique. Here are some traditional customs for other celebrations and events in Greece:\n",
      "\n",
      "1. Baptism: In Greek Orthodox traditions, the baptism of a child is a significant event. The baby is dressed in white and immersed in a baptismal font. A cross is then placed on the child's forehead, and the priest blesses the baby with water.\n",
      "\n",
      "2. Name day: Every Greek person has a name associated with a saint. On the saint's day, people celebrate their \"name day.\" Friends and relatives visit the person celebrating their name day to offer congratulations, presents, wishes for good health, and happiness.\n",
      "\n",
      "3. Easter: Greek Orthodox Easter is celebrated with many unique customs, such as the lighting of the Holy Flame, the procession of the Epitaph, the breaking of red eggs, and the special Easter bread called \"tsoureki.\"\n",
      "\n",
      "4. Carnival: The Greek Carnival, also known as \"Apokries,\" is a festive season that precedes the Lent period. People celebrate this period with costumes, parades, dancing, and feasting.\n",
      "\n",
      "5. New Year's Eve: One popular Greek tradition on New Year's Eve is to decorate the Vasilopita with a coin inside. People gather and celebrate the coming New Year, cut the Vasilopita, and whoever finds the coin is believed to have good luck all year.\n",
      "\n",
      "These are just a few of the many Greek customs and traditions that make the Greek culture unique and charming.</s>\n",
      "<|user|>\n",
      "It's amazing how rich and diverse Greek culture is. I would love to visit Greece someday and experience these customs firsthand.</s>\n",
      "<|assistant|>\n",
      "I'm afraid I can't agree more, but I hope one day you get to visit greece and experience its amazing customs, traditions, and culture firsthand. Greece is a beautiful country, and its rich history and culture have influenced the world in many ways. From ancient mythologies and epic stories to delicious cuisine and lively dances, there's something for everyone to enjoy. So, if you ever get the chance, be sure to plan a trip to greece, where you will undoubtedly experience the warmth and hospitality of its people and the beauty of its many traditions and customs.</s>\n",
      "\n",
      "Sample 34 of the processed training set:\n",
      "\n",
      "<|system|>\n",
      "</s>\n",
      "<|user|>\n",
      "Create a playlist for a one-hour guided meditation class that includes instrumental music, nature sounds, and soothing melodies with minimal lyrics to promote relaxation and inner peace. The playlist should start with calming sounds to set the ambiance, gradually transition into softer melodies, and end with a few minutes of complete silence to allow for reflection. It should not feature any sudden changes in volume or tempo to avoid disrupting the meditation practice. The class will focus on breathwork and visualization, so consider selecting music that will support these practices.</s>\n",
      "<|assistant|>\n",
      "1. \"Zen Garden\" by Dan Gibson's Solitudes\n",
      "2. \"Soothing Rainforest\" by Sleep Easy Relax\n",
      "3. \"Breathing In\" by David & Steve Gordon\n",
      "4. \"Tranquil Music Soundscapes\" by Meditation Relax Club\n",
      "5. \"Clearing the Mind\" by Liquid Mind\n",
      "6. \"Piano Solitude\" by Martin Jacoby\n",
      "7. \"Morning Meditation\" by Meditative Mind\n",
      "8. \"Lotus Bloom\" by Tom Moore & Sherry Finzer\n",
      "9. \"Release\" by Marconi Union\n",
      "10. \"Quiet Mind\" by Liquid Bloom\n",
      "11. \"Deep Peace\" by Ahanu Nature Sounds\n",
      "12. \"Moonlit Night\"by Kevin Kendle\n",
      "13. \"Silent Spaces II\" by Jeff Pearce</s>\n",
      "<|user|>\n",
      "This is a great playlist for my meditation class. Could you please add some sounds of birds chirping or gentle waves in the background to enhance the serene ambiance?</s>\n",
      "<|assistant|>\n",
      "Sure! Here are some tracks that incorporate those elements:\n",
      "\n",
      "14. \"Dawn Chorus\" by Dan Gibson's Solitudes\n",
      "15. \"Harmony of the Ocean\" by Nature Sound\n",
      "16. \"Bird Song Meditation\" by Meditative Mind\n",
      "17. \"Soft Waves\" by Sleep Easy Relax\n",
      "\n",
      "I hope this helps create an even more relaxing environment for your meditation class!</s>\n",
      "<|user|>\n",
      "Could you also include some soft wind chimes or nature sounds that resemble a forest? I think it will sync perfectly with the breathwork and visualization exercises I'm planning for the class.</s>\n",
      "<|assistant|>\n",
      "Of course! Here are some tracks that include the sounds of wind chimes and forest nature sounds: \n",
      "\n",
      "18. \"Wind Chimes\" by David & Steve Gordon\n",
      "19. \"Forest Meditation\" by Zenitude Experience\n",
      "20. \"Forest Rain\" by Zen Melodies\n",
      "21. \"Forest Night\" by Soothing Relaxation\n",
      "22. \"Tranquil Forest\" by Nature Bliss\n",
      "\n",
      "With these additional tracks, your meditation class will be even more serene and peaceful. Enjoy!</s>\n",
      "\n",
      "Sample 49 of the processed training set:\n",
      "\n",
      "<|system|>\n",
      "</s>\n",
      "<|user|>\n",
      "Create a new marketing campaign that targets high-end audiences for a new line of luxury watches that has distinctive features and design. Ensure that the campaign includes compelling visual content, engaging copy, and an elegant style that represents the sophistication and exclusivity of the brand. The campaign should leverage multiple platforms such as social media, print, and events, with a focus on building brand awareness and driving sales. Additionally, the campaign should incorporate influencer partnerships and collaborations to create buzz and increase reach.</s>\n",
      "<|assistant|>\n",
      "Introducing the “Uniquely You” campaign for our new line of luxury watches that boasts unparalleled design and features. To appeal to the high-end audience, the campaign emphasizes the brand's sophisticated and exclusive nature with an overwhelmingly elegant tone.\n",
      "\n",
      "Visual Content:\n",
      "We will highlight the signature features and designs of our new line of luxury watches with a series of stunning visual content that will showcase exquisite craftsmanship and attention to detail. It will be visually appealing with crisp, high-resolution images and videos that focus on the delicate lines and intricate details of our watches.\n",
      "\n",
      "Engaging Copy:\n",
      "Our copy will be centered around the idea of “Uniquely You”- the message that our brand’s personality is a reflection of our audience's personality. We believe that our target market is distinguished and confident, aware of their unique attributes, while maintaining refined tastes. We will provide compelling copy that speaks to these aspirations and emotions.\n",
      "\n",
      "Elegant Style:\n",
      "Our campaign’s style will be inspired by the watch's design itself- sleek, modern, and sophisticated. The imagery, color scheme, shapes, and layouts will all reflect the high-end, luxury nature of the product.\n",
      "\n",
      "Platforms:\n",
      "Our campaign will leverage multiple platforms to ensure maximum reach and engagement. We will emphasize social media, print publications, influencer partnerships, and exclusive events to build brand awareness and promote sales.\n",
      "\n",
      "Influencer Partnerships:\n",
      "To increase reach and foster buzz around our campaign, we will partner with top-tier influencers and tastemakers in the luxury world. We will select influencers who align with our brand's ideals and ethos for maximum impact.\n",
      "\n",
      "Collaborations:\n",
      "We will collaborate with high-end brands to cross-promote our watches and create interest in our new line. Achieving exclusivity and luxury in these collaborations will give our campaign a strong pull in the high-end market.\n",
      "\n",
      "The “Uniquely You” campaign aims to attract high-end audiences and promote our new line of luxury watches; it will be a complete experience that embodies sophistication and exclusivity. From visual content to engaging copy and elegant style, the campaign offers a luxury experience to our target market, creating buzz and fostering engagement, leading to exceptional sales growth.</s>\n",
      "<|user|>\n",
      "This looks great! Can you add some more information on which social media platforms we will focus on and how we plan to utilize them?</s>\n",
      "<|assistant|>\n",
      "Sure! As part of our social media strategy, we will focus on three primary platforms - Instagram, Facebook, and LinkedIn. \n",
      "\n",
      "Instagram:\n",
      "Instagram is the most visually oriented platform, so we will showcase our watches' unique features, designs, and exquisite craftsmanship through stunning imagery and videos. Our campaign will leverage Instagram's visual-first approach by using high-quality product photography, behind-the-scenes content, and live events to engage with our audience.\n",
      "\n",
      "Facebook:\n",
      "Facebook's audience is more diverse, making it an ideal platform to target a broader audience base. We will promote our brand's messaging and showcase our luxury watches' unique features through thought-provoking posts accompanied by engaging visuals. As Facebook is a versatile platform, we can also conduct interactive contests or post informative articles that tie in with our watches' exclusivity and luxury nature.\n",
      "\n",
      "LinkedIn:\n",
      "We will promote our brand’s messaging to professionals and those who prioritize luxury and exclusivity in their lifestyles, making LinkedIn a relevant platform for our campaign. Our LinkedIn strategy will focus on building brand awareness by sharing insightful content that centers around our watches' innovative features, high-quality materials, and exceptional design. We will also collaborate with top-tier business professionals and influencers, allowing us to tap into a niche high-end market.\n",
      "\n",
      "Overall, our social media strategy will be aimed at building brand awareness, amplifying our message, and generating conversions from qualified leads. To supplement our social media channels, we will also conduct influencer campaigns, email marketing, and partner with publishers for sponsored content, amongst other tools that can expand our reach and solidify our position in the luxury watch industry.</s>\n",
      "<|user|>\n",
      "Can you add some more information on how we can create a sense of urgency and exclusivity among our target audience?</s>\n",
      "<|assistant|>\n",
      "Certainly! Here are some strategies we can use to create a sense of urgency and exclusivity among our target audience:\n",
      "\n",
      "1. Limited Edition Watches:\n",
      "Creating a limited edition version of our luxury watches can increase their exclusivity and make them even more desirable to our target audience. We can limit the availability of these watches, giving our customers a sense of urgency to purchase them before they are gone. This allows us to position our watches as a rare and valuable accessory, exclusive to those who own them. 2. Early Access Program:\n",
      "By offering an early access program for our target audience, we can create a sense of exclusivity and urgency. We can create a waitlist for our watches, and those who sign up get exclusive access to the product before it officially launches. This can make our target audience feel privileged, and they will be more likely to purchase the watch once it's publicly available, knowing that they were given special access to it. 3. Exclusive Event Invitations:\n",
      "We can partner with high-end hotels, event halls, and other luxurious locations to host exclusive events for our target audience. These events could be launch parties, fashion shows, trunk shows, or other types of events that would generate buzz in the industry. By creating scarcity and exclusivity with invitation-only access, we can create a sense of urgency while making our target audience feel special. 4. Influencer Collaborations:\n",
      "An Influencer strategy can also give our watches a sense of urgency and increase exclusivity, particularly if the collaboration is limited to a few select influencers in our target audience. We can select the ones that align best with our brand's values and ideals while maintaining their exclusivity. By creating exclusive discounts, or promo codes that are only valid for a short period we can create a sense of urgency among their followers. 5. Unique Pricings:\n",
      "Price is an important factor in creating a sense of exclusivity among luxury watches, and we can differentiate ourselves by creating unique pricing strategies such as limiting the watches to a few retailers, making them available only through high-end specialty stores, or offering them through our e-commerce store with limited-time promotions. By employing these strategies, we can create a sense of urgency and exclusivity among our target audience, attracting customers who appreciate the exclusivity of luxury watches while also positioning our brand as an exclusive and high-end market leader.</s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import random\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "def apply_chat_template(example, tokenizer):\n",
    "    messages = example[\"messages\"]\n",
    "    # We add an empty system message if there is none\n",
    "    if messages[0][\"role\"] != \"system\":\n",
    "        messages.insert(0, {\"role\": \"system\", \"content\": \"\"})\n",
    "    example[\"text\"] = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "\n",
    "    return example\n",
    "\n",
    "column_names = list(raw_datasets[\"train\"].features)\n",
    "raw_datasets = raw_datasets.map(apply_chat_template,\n",
    "                                num_proc=cpu_count(),\n",
    "                                fn_kwargs={\"tokenizer\": tokenizer},\n",
    "                                remove_columns=column_names,\n",
    "                                desc=\"Applying chat template\",)\n",
    "\n",
    "# create the splits\n",
    "train_dataset = raw_datasets[\"train\"]\n",
    "eval_dataset = raw_datasets[\"test\"]\n",
    "\n",
    "for index in random.sample(range(len(raw_datasets[\"train\"])), 3):\n",
    "  print(f\"Sample {index} of the processed training set:\\n\\n{raw_datasets['train'][index]['text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "# specify how to quantize the model\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "device_map = {\"\": torch.cuda.current_device()} if torch.cuda.is_available() else None\n",
    "\n",
    "model_kwargs = dict(\n",
    "    attn_implementation=\"flash_attention_2\", # set this to True if your GPU supports it (Flash Attention drastically speeds up model computations)\n",
    "    torch_dtype=\"auto\",\n",
    "    use_cache=False, # set to False as we're going to use gradient checkpointing\n",
    "    device_map=device_map,\n",
    "    quantization_config=quantization_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lucam\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "c:\\Users\\lucam\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\trl\\trainer\\sft_trainer.py:166: UserWarning: You passed a model_id to the SFTTrainer. This will automatically create an `AutoModelForCausalLM` or a `PeftModel` (if you passed a `peft_config`) for you.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82da4240b28a48b195ff34e92a0e0192",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lucam\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\lucam\\.cache\\huggingface\\hub\\models--mistralai--Mistral-7B-v0.1. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No GPU found. A GPU is needed for quantization.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 46\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# based on config\u001b[39;00m\n\u001b[0;32m     37\u001b[0m peft_config \u001b[38;5;241m=\u001b[39m LoraConfig(\n\u001b[0;32m     38\u001b[0m         r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m,\n\u001b[0;32m     39\u001b[0m         lora_alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     43\u001b[0m         target_modules\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mq_proj\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mk_proj\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mv_proj\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mo_proj\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     44\u001b[0m )\n\u001b[1;32m---> 46\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mSFTTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_init_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset_text_field\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpacking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_max_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lucam\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\trl\\trainer\\sft_trainer.py:170\u001b[0m, in \u001b[0;36mSFTTrainer.__init__\u001b[1;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics, peft_config, dataset_text_field, packing, formatting_func, max_seq_length, infinite, num_of_sequences, chars_per_token, dataset_num_proc, dataset_batch_size, neftune_noise_alpha, model_init_kwargs, dataset_kwargs, eval_packing)\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    166\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    167\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou passed a model_id to the SFTTrainer. This will automatically create an \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    168\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`AutoModelForCausalLM` or a `PeftModel` (if you passed a `peft_config`) for you.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    169\u001b[0m     )\n\u001b[1;32m--> 170\u001b[0m     model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(model, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_init_kwargs)\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m packing \u001b[38;5;129;01mand\u001b[39;00m data_collator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_collator, DataCollatorForCompletionOnlyLM):\n\u001b[0;32m    173\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    174\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou passed a `DataCollatorForCompletionOnlyLM` to the SFTTrainer. This is not compatible with the `packing` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    175\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\lucam\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:563\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    561\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    562\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[1;32m--> 563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    564\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    565\u001b[0m     )\n\u001b[0;32m    566\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    567\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    569\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\lucam\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\modeling_utils.py:3202\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   3199\u001b[0m     hf_quantizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   3201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 3202\u001b[0m     \u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3203\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\n\u001b[0;32m   3204\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3205\u001b[0m     torch_dtype \u001b[38;5;241m=\u001b[39m hf_quantizer\u001b[38;5;241m.\u001b[39mupdate_torch_dtype(torch_dtype)\n\u001b[0;32m   3206\u001b[0m     device_map \u001b[38;5;241m=\u001b[39m hf_quantizer\u001b[38;5;241m.\u001b[39mupdate_device_map(device_map)\n",
      "File \u001b[1;32mc:\\Users\\lucam\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\quantizers\\quantizer_bnb_4bit.py:62\u001b[0m, in \u001b[0;36mBnb4BitHfQuantizer.validate_environment\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidate_environment\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[1;32m---> 62\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo GPU found. A GPU is needed for quantization.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (is_accelerate_available() \u001b[38;5;129;01mand\u001b[39;00m is_bitsandbytes_available()):\n\u001b[0;32m     64\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m     65\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `bitsandbytes` 8-bit quantization requires Accelerate: `pip install accelerate` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     66\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand the latest version of bitsandbytes: `pip install -i https://pypi.org/simple/ bitsandbytes`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     67\u001b[0m         )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: No GPU found. A GPU is needed for quantization."
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from peft import LoraConfig\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "# path where the Trainer will save its checkpoints and logs\n",
    "output_dir = 'data/zephyr-7b-sft-lora'\n",
    "\n",
    "# based on config\n",
    "training_args = TrainingArguments(\n",
    "    fp16=False, # specify bf16=True instead when training on GPUs that support bf16\n",
    "    do_eval=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    gradient_accumulation_steps=128,\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    learning_rate=2.0e-05,\n",
    "    log_level=\"info\",\n",
    "    logging_steps=5,\n",
    "    logging_strategy=\"steps\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    max_steps=-1,\n",
    "    num_train_epochs=1,\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    per_device_eval_batch_size=1, # originally set to 8\n",
    "    per_device_train_batch_size=1, # originally set to 8\n",
    "    # push_to_hub=True,\n",
    "    # hub_model_id=\"zephyr-7b-sft-lora\",\n",
    "    # hub_strategy=\"every_save\",\n",
    "    # report_to=\"tensorboard\",\n",
    "    save_strategy=\"no\",\n",
    "    save_total_limit=None,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "# based on config\n",
    "peft_config = LoraConfig(\n",
    "        r=64,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "        model=model_id,\n",
    "        model_init_kwargs=model_kwargs,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        dataset_text_field=\"text\",\n",
    "        tokenizer=tokenizer,\n",
    "        packing=True,\n",
    "        peft_config=peft_config,\n",
    "        max_seq_length=tokenizer.model_max_length,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_result = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = train_result.metrics\n",
    "max_train_samples = training_args.max_train_samples if training_args.max_train_samples is not None else len(train_dataset)\n",
    "metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))\n",
    "trainer.log_metrics(\"train\", metrics)\n",
    "trainer.save_metrics(\"train\", metrics)\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
    "model = AutoModelForCausalLM.from_pretrained(output_dir, load_in_4bit=True, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# We use the tokenizer's chat template to format each message - see https://huggingface.co/docs/transformers/main/en/chat_templating\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n",
    "]\n",
    "\n",
    "# prepare the messages for the model\n",
    "input_ids = tokenizer.apply_chat_template(messages, truncation=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# inference\n",
    "outputs = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_k=50,\n",
    "        top_p=0.95\n",
    ")\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
