{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12b55bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pygame in c:\\users\\lucam\\appdata\\roaming\\python\\python311\\site-packages (2.5.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "640eddad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium in c:\\users\\lucam\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\lucam\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gymnasium) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\lucam\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gymnasium) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\lucam\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gymnasium) (4.11.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\lucam\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gymnasium) (0.0.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install gymnasium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8db04fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import pygame\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c03e82f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import pdb\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "import random\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb866c9",
   "metadata": {},
   "source": [
    "### Environment characteristics:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f6af24",
   "metadata": {},
   "source": [
    "The environment is taken from the OpenAI Gym library. The environment used is to taken to provide the Frozen Lake game.\n",
    "\\\n",
    "\\\n",
    "The aim of this game is to cross a frozen lake, starting from an initial position, S, to a Goal position, G, walking over the frozen lake, F, which is safe, without falling into hole, H."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74826db",
   "metadata": {},
   "source": [
    "##### Action Space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604ce246",
   "metadata": {},
   "source": [
    "The agent can do 4 possible actions, navigate **left**, **right**, **up** and **down**. The game ends when the agent reaches the goal or fall in a hole.\\\n",
    "\\\n",
    "The agent takes one element vector action per time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f978c83f",
   "metadata": {},
   "source": [
    "##### State Space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc298c5",
   "metadata": {},
   "source": [
    "The state space is defined by a value which rappresent the agent current position, and it is calculeted as: *current row + current columns*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec11a7af",
   "metadata": {},
   "source": [
    "##### Reward Schedule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b7b655",
   "metadata": {},
   "source": [
    "The reward can be 0 or 1. If the agent reaches the goal, G, the reward is equal to 1, otherwise is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fad72323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "State Space: Discrete(16)\n",
      "Action Space: Discrete(4)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v1')\n",
    "print(env.reset()[0])\n",
    "print(f'State Space: {env.observation_space}')\n",
    "print(f'Action Space: {env.action_space}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a292755b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0.0, False, False, {'prob': 0.3333333333333333})\n"
     ]
    }
   ],
   "source": [
    "print(env.step(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc62e79",
   "metadata": {},
   "source": [
    "### Monte Carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b50e7828",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_policy(env):\n",
    "    policy = {}\n",
    "    for key in range(0, env.observation_space.n):\n",
    "        current_end = 0\n",
    "        p = {}\n",
    "        for action in range(0, env.action_space.n):\n",
    "            p[action] = 1 / env.action_space.n\n",
    "        policy[key] = p\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2c46dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_action_dictionary(env, policy):\n",
    "    Q = {}\n",
    "    for key in policy.keys():\n",
    "         Q[key] = {a: 0.0 for a in range(0, env.action_space.n)}\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a5aa0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_game(env, policy, display=True):\n",
    "    env.reset()\n",
    "    episode = []\n",
    "    finished = False\n",
    "\n",
    "    while not finished:\n",
    "        s = env.env.s\n",
    "        if display:\n",
    "            clear_output(True)\n",
    "            env.render()\n",
    "            sleep(1)\n",
    "\n",
    "        timestep = []\n",
    "        timestep.append(s)\n",
    "        n = random.uniform(0, sum(policy[s].values()))\n",
    "        top_range = 0\n",
    "        for prob in policy[s].items():\n",
    "                top_range += prob[1]\n",
    "                if n < top_range:\n",
    "                    action = prob[0]\n",
    "                    break \n",
    "        state, reward, finished, info, prob = env.step(action)\n",
    "        timestep.append(action)\n",
    "        timestep.append(reward)\n",
    "\n",
    "        episode.append(timestep)\n",
    "\n",
    "    if display:\n",
    "        clear_output(True)\n",
    "        env.render()\n",
    "        sleep(1)\n",
    "    return episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc2fada9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_policy(policy, env):\n",
    "    wins = 0\n",
    "    r = 100\n",
    "    for i in range(r):\n",
    "        w = run_game(env, policy, display=False)[-1][-1]\n",
    "        if w == 1:\n",
    "                wins += 1\n",
    "    return wins / r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb2a8496",
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_e_soft(env, episodes=100, policy=None, epsilon=0.01):\n",
    "    if not policy:\n",
    "        policy = random_policy(env)  # Create an empty dictionary to store state action values    \n",
    "    Q = state_action_dictionary(env, policy) # Empty dictionary for storing rewards for each state-action pair\n",
    "    returns = {}\n",
    "    \n",
    "    for _ in range(episodes): # Looping through episodes\n",
    "        G = 0 # Store cumulative reward in G (initialized at 0)\n",
    "        episode = run_game(env=env, policy=policy, display=False) # Store state, action and value respectively \n",
    "        \n",
    "        # for loop through reversed indices of episode array. \n",
    "        # The logic behind it being reversed is that the eventual reward would be at the end. \n",
    "        # So we have to go back from the last timestep to the first one propagating result from the future.\n",
    "        \n",
    "        for i in reversed(range(0, len(episode))):   \n",
    "            s_t, a_t, r_t = episode[i] \n",
    "            state_action = (s_t, a_t)\n",
    "            G += r_t # Increment total reward by reward on current timestep\n",
    "            \n",
    "            if not state_action in [(x[0], x[1]) for x in episode[0:i]]: # \n",
    "                if returns.get(state_action):\n",
    "                    returns[state_action].append(G)\n",
    "                else:\n",
    "                    returns[state_action] = [G]   \n",
    "                    \n",
    "                Q[s_t][a_t] = sum(returns[state_action]) / len(returns[state_action]) # Average reward across episodes\n",
    "                \n",
    "                Q_list = list(map(lambda x: x[1], Q[s_t].items())) # Finding the action with maximum value\n",
    "                indices = [i for i, x in enumerate(Q_list) if x == max(Q_list)]\n",
    "                max_Q = random.choice(indices)\n",
    "                \n",
    "                A_star = max_Q\n",
    "                \n",
    "                for a in policy[s_t].items(): # Update action probability for s_t in policy\n",
    "                    if a[0] == A_star:\n",
    "                        policy[s_t][a[0]] = 1 - epsilon + (epsilon / abs(sum(policy[s_t].values())))\n",
    "                    else:\n",
    "                        policy[s_t][a[0]] = (epsilon / abs(sum(policy[s_t].values())))\n",
    "\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c366d5f3",
   "metadata": {},
   "source": [
    "#### Run the MC algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ec478c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lucam\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.s to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.s` for environment variables or `env.get_wrapper_attr('s')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.57"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy = monte_carlo_e_soft(env, episodes=1000)\n",
    "test_policy(policy, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce980895",
   "metadata": {},
   "source": [
    "### Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9cd8053",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q_learn:\n",
    "\n",
    "  def __init__(self,env):\n",
    "    self.env=env  # Env already defined\n",
    "\n",
    "\n",
    "  def QTable(self):\n",
    "    self.q_table = np.zeros([self.env.observation_space.n,self.env.action_space.n])\n",
    "    return self.q_table\n",
    "\n",
    "\n",
    "  def train(self,epoch=100001,alpha = 0.1,gamma = 0.6,epsilon = 0.1):\n",
    "    self.QTable()\n",
    "    for i in range(1, epoch):\n",
    "        # Reset the action made in the environments\n",
    "        state = self.env.reset()[0]\n",
    "        # Sure that the agent is not in G\n",
    "        done = False\n",
    "        # Impose the reward and epochs null\n",
    "        epochs, reward = 0, 0\n",
    "        \n",
    "\n",
    "        while not done:\n",
    "            if random.uniform(0, 1) < epsilon:\n",
    "                # We made exploration of the action space (the action provide will be random in such a way to explore a random state)\n",
    "                action = self.env.action_space.sample()\n",
    "            else:\n",
    "                # Exploitation --> get the action which provide the maximum Q-value in the current state\n",
    "                action = np.argmax(self.q_table[state])\n",
    "\n",
    "            # Update the agent with the new info getting after provide the action    \n",
    "            next_state, reward, done, info, prob = self.env.step(action) \n",
    "            \n",
    "            # Update the Q-table using the Bellman equation\n",
    "            old_value = self.q_table[state, action] # Old Q-table value\n",
    "            next_max = np.max(self.q_table[next_state]) # learned value\n",
    "            \n",
    "            new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "            self.q_table[state, action] = new_value\n",
    "\n",
    "            state = next_state\n",
    "            epochs += 1\n",
    "            \n",
    "        if i % 100 == 0:\n",
    "            clear_output(wait=True)\n",
    "            print(f\"Episode: {i}\")\n",
    "\n",
    "    print(\"Training finished.\\n\")\n",
    "\n",
    "  def trainTune(self,alpha = 0.5,gamma = 0.8,epsilon = 0.6):\n",
    "    self.QTable()\n",
    "    for i in range(1, 100001):\n",
    "        state = self.env.reset()[0]\n",
    "        epochs, reward, = 0, 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            if random.uniform(0, 1) < epsilon:\n",
    "                action = self.env.action_space.sample() # Explore action space\n",
    "            else:\n",
    "                action = np.argmax(self.q_table[state]) # Exploit learned values\n",
    "\n",
    "            next_state, reward, done, info, prob = self.env.step(action) \n",
    "            \n",
    "            old_value = self.q_table[state, action]\n",
    "            next_max = np.max(self.q_table[next_state])\n",
    "            \n",
    "            new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "            self.q_table[state, action] = new_value\n",
    "\n",
    "            state = next_state\n",
    "            epochs += 1\n",
    "            \n",
    "        if i % 100 == 0:\n",
    "            clear_output(wait=True)\n",
    "            print(f\"Episode: {i}\")\n",
    "        if i % 1000 == 0:\n",
    "            epsilon-=0.0058\n",
    "            gamma-=0.0078\n",
    "            alpha-=0.0048\n",
    "            print(epsilon,gamma,alpha,sep='\\n')\n",
    "\n",
    "    print(\"Training finished.\\n\")\n",
    "  def trainGraidSearch(self,hyper):\n",
    "    self.QTable()\n",
    "    df=pd.DataFrame(columns=['epoch','alpha','gama','epsilon','average Time'])\n",
    "    epochF=False\n",
    "    alphaF=False\n",
    "    gammaF=False\n",
    "    epsilonF=False\n",
    "    try:\n",
    "      epochs=hyper['epochs']\n",
    "      epochF=True\n",
    "    except:\n",
    "      epochs=[100001]\n",
    "    try:\n",
    "      alphas=hyper['alphas']\n",
    "      alphaF=False\n",
    "    except:\n",
    "      alphas=[0.1]\n",
    "    try:\n",
    "      gammas=hyper['gammas']\n",
    "      gammaF=True\n",
    "    except:\n",
    "      gammas=[0.6]\n",
    "    try:\n",
    "      epsilons=hyper['epsilons']\n",
    "      epsilonF=True\n",
    "    except:\n",
    "      epsilons=[0.1]\n",
    "    first=True\n",
    "    best_parameter=[]\n",
    "    for epoch in epochs:\n",
    "      for alpha in alphas:\n",
    "        for gamma in gammas:\n",
    "          for epsilon in epsilons:\n",
    "            self.train(epoch,alpha,gamma,epsilon)\n",
    "            average=self.evaloute()\n",
    "            df.loc[len(df.index)]=[epoch,alpha,gamma,epsilon,average]\n",
    "            display(df)\n",
    "    best_parmeter=df.loc[df['average Time'].idxmin()]\n",
    "    return best_parmeter \n",
    "\n",
    "\n",
    "  def evaloute(self,episodes = 100):\n",
    "    total_epochs= 0\n",
    "    for _ in range(episodes):\n",
    "        state = self.env.reset()[0]\n",
    "        epochs, reward = 0, 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            action = np.argmax(self.q_table[state])\n",
    "            state, reward, done, info, prob = self.env.step(action)\n",
    "            epochs += 1\n",
    "        total_epochs += epochs\n",
    "    average=total_epochs / episodes\n",
    "    print(f\"Results after {episodes} episodes:\")\n",
    "    print(f\"Average timesteps per episode: {average}\")\n",
    "    return average\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf587c74",
   "metadata": {},
   "source": [
    "Train the Agent using Q-learning: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9afa675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 100000\n",
      "Training finished.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "RL_model3=Q_learn(env)\n",
    "RL_model3.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "baf57767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results after 100 episodes:\n",
      "Average timesteps per episode: 10.92\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10.92"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RL_model3.evaloute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3f591ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 100000\n",
      "0.019999999999998352\n",
      "0.01999999999999974\n",
      "0.019999999999998685\n",
      "Training finished.\n",
      "\n",
      "Results after 100 episodes:\n",
      "Average timesteps per episode: 5.33\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5.33"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RL_model3.trainTune()\n",
    "RL_model3.evaloute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4264b1",
   "metadata": {},
   "source": [
    "### Deep Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "afa8cf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "lr = 0.85\n",
    "gamma = 0.99\n",
    "num_episodes = 2000\n",
    "num_iterations = 200\n",
    "rewards = np.zeros(num_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c872175",
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in range(num_episodes):\n",
    "    state = env.reset()[0]\n",
    "    for iteration in range(num_iterations):\n",
    "            action = np.argmax(Q_table[state,:] + np.random.randn(1, env.action_space.n)*(1./(episode+1)))\n",
    "            next_state, reward, done, info, prob = env.step(action) \n",
    "            \n",
    "            Q_table[state, action] = Q_table[state, action] + lr*(reward + gamma*np.max(Q_table[next_state,:]) - Q_table[state, action])\n",
    "            state = next_state\n",
    "            \n",
    "            if done or iteration == num_iterations-1:\n",
    "                rewards[episode] = reward\n",
    "            if done:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e9c59f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_conseq_max():\n",
    "    sum_cur = sum(rewards[0:100])\n",
    "    max_ = sum_cur\n",
    "    for i in range(100, num_episodes):\n",
    "        sum_cur += rewards[i] - rewards[i-100]\n",
    "        max_ = max(max_, sum_cur)\n",
    "    return max_/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a90c1c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.84\n"
     ]
    }
   ],
   "source": [
    "print(find_conseq_max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ad34e241",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155b0f33",
   "metadata": {},
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self):\n",
    "        self.state_dim = 3\n",
    "        self.action_dim = 9\n",
    "        self.lr = 0.01\n",
    "        self.gamma = 0.98\n",
    "        self.tau = 0.01\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_decay = 0.98\n",
    "        self.epsilon_min = 0.001\n",
    "        self.buffer_size = 100000\n",
    "        self.batch_size = 200\n",
    "        self.memory = ReplayBuffer(self.buffer_size)\n",
    "        \n",
    "        self.Q = Qnetwork(self.state_dim, state.action_dim, state.lr)\n",
    "        self.Qtarget = Qnetwork(self.state_dim, state.action_dim, state.lr)\n",
    "        self.Qtarget.load_state_dict(self.Q.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de01552",
   "metadata": {},
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_limit):\n",
    "        self.buffer = duque(maxlen=buffer_limit)\n",
    "        \n",
    "    def put(self, transition):\n",
    "        self.buffer.append(transation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "843ac157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the neural network\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DQNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(DQNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, action_dim)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4f1a45a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    def __init__(self, maxlen):\n",
    "        self.memory = deque([], maxlen=maxlen)\n",
    "    \n",
    "    def append(self, transition):\n",
    "        self.memory.append(transition)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)  \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dc60f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrozenLake():\n",
    "    lr = 0.001 # Learning rate\n",
    "    epsilon = 0.9 # Exploration rate\n",
    "    gamma = 0.9 # Discount factor\n",
    "    update_rate = 10 # Update target network every 10 episodes\n",
    "    replay_memory_size = 1000 # Replay memory size\n",
    "    batch_size = 64 # Number of samples to take from replay memory\n",
    "\n",
    "    ACTIONS = ['L', 'D', 'R', 'U'] # Left, Down, Right, Up\n",
    "\n",
    "    def train(self, episodes, render=True, is_slippery=False, map_dim):\n",
    "        if map_dim = 4:\n",
    "            env = gym.make('FrozenLake-v1', map_name='4x4', is_slippery=is_slippery, render_mode='human' if render else None)\n",
    "        else:\t\n",
    "            env = gym.make('FrozenLake-v1', map_name='8x8', is_slippery=is_slippery, render_mode='human' if render else None)\n",
    "        states = env.observation_space.n\n",
    "        actions = env.action_space.n\n",
    "\n",
    "    memory = ReplayMemory(self.replay_memory_size)\n",
    "\n",
    "    policy_net = DQNetwork(states, actions)\n",
    "    target_net = DQNetwork(states, actions)\n",
    "\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "    print('Training on', 'slippery' if is_slippery else 'non-slippery', 'FrozenLake')\n",
    "    print('Policy (random, before training):')\n",
    "\n",
    "    self.print_dqn(policy_net)\n",
    "\n",
    "    loss = nn.MSELoss()\n",
    "    self.optimizer = torch.optim.Adam(policy_net.parameters(), lr=self.lr)\n",
    "\n",
    "    rewards_per_episode = np.zeros(episodes)\n",
    "\n",
    "    epsilon_history = []\n",
    "\n",
    "    steps = 0\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()[0]\n",
    "        done = False # True if agent stepped on a hole or reached the goal\n",
    "        stop = False # True if agent takes too long to solve the environment\n",
    "\n",
    "        while not done and not stop:\n",
    "            # Epsilon-greedy policy\n",
    "\n",
    "            if random.random() < self.epsilon:\n",
    "                # take a random action\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                # take the action with the highest Q-value\n",
    "                with torch.no_grad(): # Disable gradient tracking since we are not training\n",
    "                    action = policy_net(self.state_to_dqn_input(state, states)).argmax().item()\n",
    "            \n",
    "            new_state, reward, done,_ = env.step(action)\n",
    "            memory.append((state, action, reward, new_state, done))\n",
    "\n",
    "            state = new_state\n",
    "            steps += 1\n",
    "\n",
    "        if reward == 1:\n",
    "            rewards_per_episode[episode] = 1\n",
    "\n",
    "        if len(memory) >= self.batch_size and np.sum(rewards_per_episode) > 0:\n",
    "            batch = memory.sample(self.batch_size) # Sample a batch of transitions\n",
    "            self.optimize(policy_net, target_net, batch) # Optimize the policy network passing the batch\n",
    "\n",
    "            epsilon = max(epsilon - 1/episodes, 0)\n",
    "            epsilon_history.append(epsilon)\n",
    "\n",
    "            if steps > self.update_rate:\n",
    "                target_net.load_state_dict(policy_net.state_dict())\n",
    "                steps = 0\n",
    "    \n",
    "\n",
    "    env.close()\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6950b92e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
